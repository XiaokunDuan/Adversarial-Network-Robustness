[{"content":"Motivation Transformer-based LLMs and other trendy NNs has achieve significant successes. It seems like the power of AI dominates almost every field. The efficiency and accuracy outnumber the human intelligence completely. The human last exam, including various difficult intelligence puzzles, has beaten by AI recently at a extremely high level.\nHowever, the seemingly powerful tools demonstrates staggering fragility when facing a small disturbance. [example] [fig]\nThese treats can be classified into several categories.[classes][fig]\nWhy robustness matters?\nThe treats can be pretty sensitive under circumstances of self-driving, medical inspect and military affairs.[example][fig]\nAdversarial Training and Empirical Defenses A classic paradox is that what happens when an unstoppable force meets an immovable object?\n[limitations]\n(Zhang et al., 2023) Left columns are original point cloud, while the right columns are disturbed slightly. The model falls into Type I and Type II errors. Image source: (Altekrüger et al., 2024)\nBelow is the literature review concentrating on 3D point clouds.[literature review]\nTargeted removal：An adversarial attack can identify the most critical points for a 3D detector using a gradient-based saliency map. By strategically removing just these few key points, the attack effectively sabotages the model\u0026rsquo;s perception.\nAn example of a targeted removal attack.\nObjective function: $X^a$ is adversarial example. $D$ is $L_2$ norm. $$ \\min J(X^a, G) = -L_{det}(X^a, G) + \\lambda D(X, X^a) $$ Iterative Attack Update Rule: $$ X_a^{t+1} = \\text{Clip}_{X, \\epsilon} \\left\\{ X_a^t - \\alpha \\frac{\\nabla_{X_a^t} J(X_a^t, G)}{|\\nabla_{X_a^t} J(X_a^t, G)|_2} \\right\\} $$ Visualization of the iterative attack update process.\nAdverasril training: balace training from my point of view $$ L_{total} = w_{clean} \\times L_{clean} + w_{adv} \\times L_{adv} $$ Quality dataset behaves strong robustness:\nAccuracy vs. Robustness plot for different 3D object detectors.\nKey takeway:Accuracy ≠ Robustness: High accuracy does not guarantee high robustness. Voxel-based models are stronger: Red markers cluster in the top-right, showing better overall performance. Because of gradient disruption and homogeneous perturbation.（rasterization /pixelization）\nHowever, we cannot conclusively state that voxel-based detectors are more robust.\nPlot Explanation \u0026ndash; X-axis (Clean mAP): Model accuracy on clean data. Further right is better. Y-axis (mAP Ratio): Model robustness under attack. Higher is better. Colors: Red = voxel-based, Green = point-based, Yellow = point-voxel hybrid. Marker Size: Represents the network size (more parameters).\nTheoretical viewpoint: From Empirical Defenses to Provable Robustness [from math perspective]\nUnstable jump: Learning the entire posterior distribution is more robust than using point estimates (like the MAP estimator). Deep learning models learn the posterior distribution by minimizing a loss function that is averaged over the entire training dataset. But good performance on average does not guarantee good performance at a single, specific point that we actually care about in practice. (Altekrüger et al., 2024) provides the first theoretical link, proving that if the average loss is sufficiently small, then the performance at a single point can also be guaranteed.\nImage source: (Altekrüger et al., 2024)\nProof logic: $$ W_1(P_{X|Y=\\tilde{y}}, G_{\\theta}(\\tilde{y}, \\cdot)_{\\#}P_Z) \\le C\\epsilon^{\\frac{1}{n+1}} $$ LHS is the $W_1$ difference between two distribution. RHS is the upper bound of certification.\nConceptual flow of the proof for establishing a robustness certificate.\nFirst, because the model\u0026rsquo;s average error across all possible data is known to be small, it is impossible for the error to be large everywhere; this guarantees the existence of at least one \u0026ldquo;well-behaved\u0026rdquo; point, $\\hat{y}$, within a small neighborhood of our specific target point, $\\tilde{y}$, where the local error is also provably small. Second, the crucial property of Lipschitz continuity is leveraged, which ensures the system is smooth and prevents abrupt changes. This means that since $\\tilde{y}$ and $\\hat{y}$ are physically close, the error at $\\tilde{y}$ cannot be drastically different from the error at $\\hat{y}$. Finally, by combining the known small error at the \u0026ldquo;well-behaved\u0026rdquo; point $\\hat{y}$ with the bounded change in error between $\\hat{y}$ and $\\tilde{y}$, the proof establishes a definitive upper bound on the model\u0026rsquo;s error at the single, specific point $\\tilde{y}$ that we care about.\n[theoretical path]\nThe core conflict persists in the research field is the trade-off between model accuracy and robustness.\nWrap-up of robustness The measurement of robustness The essence of robustness trade-off The boundary of robustness (Enomoto, 2024) Shannon Entropy： $$ H = - \\sum_{y=1}^{C} \\hat{p}(y|x) \\log \\hat{p}(y|x) $$\nThe hardest clean examples (i.e., those with high entropy) are as difficult for the model as the easiest augmented examples. The change is continuous rather than a leap.![comparision of samples](./comparision of samples-7487819.png)\nEntProp treats the augmented sample as in-distribution domain and feeds it to MBN (orange bracket ). EntProp then adversarial attacks high-entropy samples and feeds it to ABN (blue bracket ). For CIFAR10, the analysis results in an upper bound (of certified robust accuracy) of 67.49%, meanwhile existing approaches are only able to increase it from 53.89% in 2017 to 62.84% in 2023.\nVisualization of the two different trainning.\nThe Demand for Robustness: We require that a model\u0026rsquo;s prediction remains constant within a local neighborhood ($\\mathcal{V}_x$) around each data point $x$. The convolution process inherently increases the overlap between class distributions, making the problem harder. Therefore, the Bayes error of the new distribution is greater than or equal to that of the original. This requirement effectively changes the target data distribution. The problem is no longer about fitting the original distribution $\\mathcal{D}$.\nThe pursuit of certified robustness forces us to optimize for a new, inherently harder problem ($\\mathcal{D\u0026rsquo;}$) that has a higher minimum error rate. This establishes a fundamental theoretical limit on certified robust accuracy, explaining why it is inevitably lower than standard accuracy.\nCurrent Challenges Future Works References [1]Zhang, Y., Hou, J., \u0026amp; Yuan, Y. \"A Comprehensive Study of the Robustness for LiDAR-based 3D Object Detectors against Adversarial Attacks\" arXiv preprint arXiv:2212.10230 (2023). [2]Altekr\u0026uuml;ger, F., Hagemann, P., \u0026amp; Steidl, G. \"Conditional Generative Models Are Provably Robust: Pointwise Guarantees for Bayesian Inverse Problems\" arXiv preprint arXiv:2303.15845 (2024). [3]Enomoto, S. \"EntProp: High Entropy Propagation for Improving Accuracy and Robustness\" arXiv preprint arXiv:2405.18931 (2024). ","permalink":"https://github.com/XiaokunDuan/Adversarial-Network-Robustness/posts/2025-09-08-adversary-robustness/","summary":"\u003ch1 id=\"motivation\"\u003eMotivation\u003c/h1\u003e\n\u003cp\u003eTransformer-based LLMs and other trendy NNs has achieve significant successes. It seems like the power of AI dominates almost every field. The efficiency and accuracy outnumber the human intelligence completely. The human last exam, including various difficult intelligence puzzles, has beaten by AI recently at a extremely high level.\u003c/p\u003e\n\u003cp\u003eHowever, the seemingly powerful tools demonstrates staggering fragility when facing a small disturbance. [example]\n[fig]\u003c/p\u003e\n\u003cp\u003eThese treats can be classified into several categories.[classes][fig]\u003c/p\u003e","title":"Adversarial Robustness in 3D Point Clouds"}]